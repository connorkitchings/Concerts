{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbf04f",
   "metadata": {},
   "source": [
    "# Lists and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1c8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Unknown Dates To Update Links\n",
    "special_cases = {\n",
    "    ('00/00/85', 'A-Frame, Weymanda Court, Athens, GA'): 'http://everydaycompanion.com/setlists/19850000a.asp',\n",
    "    ('00/00/86', 'Phi Delta Theta House, University of Georgia, Athens, GA'): 'NODATA',\n",
    "    ('00/00/86', '** UNKNOWN **, ** UNKNOWN **, UU'): 'NODATA',\n",
    "    ('03/00/86', 'Uptown Lounge, Athens, GA'): 'http://everydaycompanion.com/setlists/19860300a.asp',\n",
    "    ('04/00/86', '** UNKNOWN **, Atlanta, GA'): 'http://everydaycompanion.com/setlists/19860400a.asp',\n",
    "    ('07/00/86', 'Washington Park, Macon, GA'): 'http://everydaycompanion.com/setlists/19860700a.asp',\n",
    "    ('01/00/87', '40 Watt Club, Athens, GA'): 'http://everydaycompanion.com/setlists/19870100a.asp',\n",
    "    ('10/00/87', 'Uptown Lounge, Athens, GA'): 'http://everydaycompanion.com/setlists/19871000a.asp',\n",
    "    ('00/00/87', '** UNKNOWN **, ** UNKNOWN **, UU'): 'http://everydaycompanion.com/setlists/19870000a.asp',\n",
    "    ('09/00/88', 'Phi Kappa Phi House, Presbyterian College, Clinton, SC'): 'NODATA',\n",
    "    ('09/00/88', \"O'Neilly's Pub, Macon, GA\"): 'http://everydaycompanion.com/setlists/19880900a.asp',\n",
    "    ('00/00/89', \"W.C. Don's, Jackson, MS\"): 'http://everydaycompanion.com/setlists/19890000a.asp',\n",
    "    ('01/00/89', 'Phi Delta Theta House, University of Georgia, Athens, GA'): 'http://everydaycompanion.com/setlists/19890100a.asp',\n",
    "    ('04/00/89', 'Sigma Alpha Epsilon House, Tuscaloosa, AL'): 'http://everydaycompanion.com/setlists/19890400b.asp',\n",
    "    ('05/00/89', 'The Brewery, Raleigh, NC'): 'http://everydaycompanion.com/setlists/19890500a.asp',\n",
    "    ('09/00/89', \"Edgar's Campus Bar, Clemson University, Clemson, SC\"): 'http://everydaycompanion.com/setlists/19890900a.asp',\n",
    "    ('10/00/89', 'Elmo House, Charlottesville, VA'): 'http://everydaycompanion.com/setlists/19891000a.asp',\n",
    "    ('00/00/90', \"Johnny D's, Somerville, MA\"): 'http://everydaycompanion.com/setlists/19900000a.asp',\n",
    "    ('08/00/90', 'Excelsior Mill, Atlanta, GA'): 'http://everydaycompanion.com/setlists/19900800a.asp',\n",
    "    ('00/00/91', 'Hollins University, Roanoke, VA'): 'NODATA',\n",
    "    ('03/21/87', 'The Rookery, Macon, GA'): 'NODATA',\n",
    "    ('02/15/24', 'Chicago Theatre, Chicago, IL'): 'NODATA',\n",
    "    ('02/16/24', 'Chicago Theatre, Chicago, IL'): 'NODATA',\n",
    "    ('02/17/24', 'Chicago Theatre, Chicago, IL'): 'NODATA',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2766b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Shows in One Day (change Link To 'b')\n",
    "multiple_shows = {\n",
    "    ('07/21/91', 'Sheridan Opera House, Telluride, CO'): 'http://everydaycompanion.com/setlists/19910721b.asp',\n",
    "    ('08/20/91', \"Toad's Place, New Haven, CT\"): 'http://everydaycompanion.com/setlists/19910820b.asp',\n",
    "    ('09/21/92', \"Woodsmen of the World Hall, Eugene, OR\"): 'http://everydaycompanion.com/setlists/19920921b.asp',\n",
    "    ('02/23/93', \"Newport Music Hall, Columbus, OH\"): 'http://everydaycompanion.com/setlists/19930223b.asp',\n",
    "    ('05/03/93', \"First Avenue, Minneapolis, MN\"): 'http://everydaycompanion.com/setlists/19930503b.asp',\n",
    "    ('05/12/93', \"Horizontal Boogie Bar, Rochester, NY\"): 'http://everydaycompanion.com/setlists/19930512b.asp',\n",
    "    ('05/15/93', \"Avalon, Boston, MA\"): 'http://everydaycompanion.com/setlists/19930515b.asp',\n",
    "    ('03/15/94', \"Avalon, Boston, MA\"): 'http://everydaycompanion.com/setlists/19940315b.asp',\n",
    "    ('07/14/94', \"The Vic Theatre, Chicago, IL\"): 'http://everydaycompanion.com/setlists/19940714b.asp',\n",
    "    ('11/05/94', \"Arnold Hall, US Air Force Academy, Colorado Springs, CO\"): 'http://everydaycompanion.com/setlists/19941105b.asp',\n",
    "    ('11/06/94', \"Theater, Lory Student Center, Colorado State University, Fort Collins, CO\"): 'http://everydaycompanion.com/setlists/19941106b.asp',\n",
    "    ('11/11/94', \"Roseland Theater, Portland, OR\"): 'http://everydaycompanion.com/setlists/19941111b.asp',\n",
    "    ('03/25/95', \"Michigan State University Auditorium, East Lansing, MI\"): 'http://everydaycompanion.com/setlists/19950325b.asp',\n",
    "    ('04/08/95', \"Irving Plaza, New York, NY\"): 'http://everydaycompanion.com/setlists/19950408b.asp',\n",
    "    ('05/06/95', \"Chastain Park, Atlanta, GA\"): 'http://everydaycompanion.com/setlists/19950506b.asp',\n",
    "    ('07/14/95', \"Cain's Main Street Stage, Tulsa, OK\"): 'http://everydaycompanion.com/setlists/19950714b.asp',\n",
    "    ('07/18/95', \"Alberta Bair Theater, Billings, MT\"): 'http://everydaycompanion.com/setlists/19950718b.asp',\n",
    "    ('07/22/95', \"Roseland Theater, Portland, OR\"): 'http://everydaycompanion.com/setlists/19950722b.asp',\n",
    "    ('07/29/95', \"Snow King Center, Jackson, WY\"): 'http://everydaycompanion.com/setlists/19950729b.asp',\n",
    "    ('04/12/97', \"Backyard, Bee Cave, TX\"): 'http://everydaycompanion.com/setlists/19970412b.asp',\n",
    "    ('09/16/97', \"Virginia Theater, Champaign, IL\"): 'http://everydaycompanion.com/setlists/19970916b.asp',\n",
    "    ('09/17/97', \"Shryock Auditorium, Southern Illinois University, Carbondale, IL\"): 'http://everydaycompanion.com/setlists/19970917b.asp',\n",
    "    ('03/19/98', \"Chesterfield Caf√©, Paris, FR\"): 'http://everydaycompanion.com/setlists/19980319b.asp',\n",
    "    ('07/01/99', \"House of Blues, West Hollywood, CA\"): 'http://everydaycompanion.com/setlists/19990701b.asp',\n",
    "    ('09/30/99', \"Backyard, Bee Cave, TX\"): 'http://everydaycompanion.com/setlists/19990930b.asp',\n",
    "    ('11/17/99', \"Orpheum Theater, Boston, MA\"): 'http://everydaycompanion.com/setlists/19991117b.asp',\n",
    "    ('07/30/00', \"Alpine Stage, Bolton Valley Resort, Bolton, VT\"): 'http://everydaycompanion.com/setlists/20000730b.asp',\n",
    "    ('07/21/01', \"Harbor Center, Portsmouth, VA\"): 'http://everydaycompanion.com/setlists/20010721b.asp',\n",
    "    ('10/16/01', \"Paramount Theater, Seattle, WA\"): 'http://everydaycompanion.com/setlists/20011016b.asp',\n",
    "    ('10/24/01', \"KGSR 107.1FM Studios, Austin, TX\"): 'http://everydaycompanion.com/setlists/20011024b.asp',\n",
    "    ('10/24/01', \"Frank Erwin Center, Austin, TX\"): 'http://everydaycompanion.com/setlists/20011024c.asp',\n",
    "    ('11/01/01', \"Roy Wilkins Civic Auditorium, St. Paul, MN\"): 'http://everydaycompanion.com/setlists/20011101b.asp',\n",
    "    ('11/08/01', \"Orpheum Theater, Boston, MA\"): 'http://everydaycompanion.com/setlists/20011108b.asp',\n",
    "    ('04/11/03', \"UIC Pavilion, Chicago, IL\"): 'http://everydaycompanion.com/setlists/20030411b.asp',\n",
    "    ('07/16/03', \"Harbor Center, Portsmouth, VA\"): 'http://everydaycompanion.com/setlists/20030716b.asp',\n",
    "    ('07/22/03', \"Paolo Soleri, Santa Fe, NM\"): 'http://everydaycompanion.com/setlists/20030722b.asp',\n",
    "    ('10/03/03', \"Backyard, Bee Cave, TX\"): 'http://everydaycompanion.com/setlists/20031003b.asp',\n",
    "    ('04/08/05', \"Chicago Theatre, Chicago, IL\"): 'http://everydaycompanion.com/setlists/20050408b.asp',\n",
    "    ('04/14/05', \"Radio City Music Hall, New York, NY\"): 'http://everydaycompanion.com/setlists/20050414b.asp',\n",
    "    ('08/01/06', \"The Palace Theatre, Louisville, KY\"): 'http://everydaycompanion.com/setlists/20060801b.asp',\n",
    "    ('11/02/06', \"Backyard, Bee Cave, TX\"): 'http://everydaycompanion.com/setlists/20061102b.asp',\n",
    "    ('04/29/10', \"Howlin' Wolf, New Orleans, LA\"): 'http://everydaycompanion.com/setlists/20100429b.asp',\n",
    "    ('06/24/10', \"Twist and Shout Records, Denver, CO\"): 'http://everydaycompanion.com/setlists/20100624b.asp',\n",
    "    ('07/26/10', \"Tennessee Theater, Knoxville, TN\"): 'http://everydaycompanion.com/setlists/20100726b.asp',\n",
    "    ('10/04/10', \"Ryman Auditorium, Nashville, TN\"): 'http://everydaycompanion.com/setlists/20101004b.asp',\n",
    "    ('04/17/13', \"Palace Theater, Louisville, KY\"): 'http://everydaycompanion.com/setlists/20130417b.asp',\n",
    "    ('01/25/19', \"Hard Rock Hotel and Casino, Riviera Maya, MX\"): 'http://everydaycompanion.com/setlists/20190125b.asp',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1fdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_corrections = {\n",
    "    ('ADAMS CENTER', 'MT'): 'ADAMS FIELDHOUSE, UNIVERSITY OF MONTANA',\n",
    "    ('AUDITORIUM THEATRE', 'CHICAGO'): 'AUDITORIUM THEATER, ROOSEVELT UNIVERSITY',\n",
    "    ('BAYFRONT ARENA', 'FL'): 'BAYFRONT AUDITORIUM',\n",
    "    ('FLEET PAVILION', 'BOSTON'): \"CAESAR'S TAHOE\",\n",
    "    (\"CAESAR'S TAHOE SHOWROOM\", 'NV'): \"CAESAR'S TAHOE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045ecd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_corrections = {\n",
    "    '23 EAST CABARET': 'PHILADELPHIA',\n",
    "    \"CAESAR'S TAHOE\": 'LAKE TAHOE',\n",
    "    'CYNTHIA WOODS MITCHELL PAVILLION': 'THE WOODLANDS',\n",
    "    'N. LITTLE ROCK': 'LITTLE ROCK',\n",
    "    'NORTH LITTLE ROCK': 'LITTLE ROCK',\n",
    "    'MT. CRESTED BUTTE': 'CRESTED BUTTE',\n",
    "    'SNOWMASS VILLAGE': 'SNOWMASS',\n",
    "    'ELON COLLEGE': 'ELON',\n",
    "    'N. MYRTLE BEACH': 'MYRTLE BEACH'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380297f8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date_for_link(date_str):\n",
    "    try:\n",
    "        dt = datetime.strptime(date_str, \"%m/%d/%y\")\n",
    "        return dt.strftime(\"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        # Handle special cases like \"00/00/85\"\n",
    "        if date_str.startswith(\"00/00/\"):\n",
    "            return date_str[-2:] + \"0000\"\n",
    "        elif date_str.startswith(\"00/\"):\n",
    "            month = date_str[3:5]\n",
    "            year = date_str[-2:]\n",
    "            return f\"19{year}{month}00\"\n",
    "        return date_str.replace(\"/\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c017af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_link(row):\n",
    "    key = (row['date'], row['venue'])\n",
    "    all_special_cases = {**special_cases, **multiple_shows}\n",
    "    if key in all_special_cases:\n",
    "        return all_special_cases[key]\n",
    "    return row['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_radio_show(venue_full):\n",
    "    if pd.isna(venue_full):\n",
    "        return 0\n",
    "    if re.search(r'\\b\\d+\\.\\d+FM\\b', venue_full):\n",
    "        return 1\n",
    "    if re.search(r'\\b\\d+\\.\\d\\b', venue_full):\n",
    "        return 1\n",
    "    radio_terms = ['NBC STUDIOS', 'ED SULLIVAN THEATER', 'STUDIO 6B', 'CNN STUDIOS', ' STUDIO', ' RECORD']\n",
    "    if any(term in venue_full for term in radio_terms):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87676ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_venue_name(row):\n",
    "    key = (row['venue_name'], row['state'])\n",
    "    if key in venue_corrections:\n",
    "        return venue_corrections[key]\n",
    "    key2 = (row['venue_name'], row['city'])\n",
    "    if key2 in venue_corrections:\n",
    "        return venue_corrections[key2]\n",
    "    return row['venue_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_city(row):\n",
    "    if row['venue_name'] in city_corrections:\n",
    "        return city_corrections[row['venue_name']]\n",
    "    if row['city'] in city_corrections:\n",
    "        return city_corrections[row['city']]\n",
    "    return row['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dim(st_yr=1986, end_yr=2024):\n",
    "    \"\"\"Process dimension table for shows\"\"\"\n",
    "    base_url = 'http://everydaycompanion.com/'\n",
    "    tour_list = [year for year in range(st_yr, end_yr + 1) if year != 2004]  # Skip 2004\n",
    "    tour_df_list = []\n",
    "    \n",
    "    for year in tour_list:\n",
    "        yr = str(year)[-2:]\n",
    "        year_link = f\"{base_url}asp/tour{yr}.asp\"\n",
    "        print(year_link)\n",
    "        \n",
    "        response = requests.get(year_link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='latin1')\n",
    "        tour_string = soup.find('p').get_text(strip=True).replace(\"??\", \"00\")\n",
    "        \n",
    "        # Split the string into individual dates and venues\n",
    "        venues = re.split(r'\\d{2}/\\d{2}/\\d{2}\\?*', tour_string)\n",
    "        venues = [venue.strip() for venue in venues if venue.strip() != \"\"]\n",
    "        dates = re.findall(r'\\d{2}/\\d{2}/\\d{2}\\b\\?*', tour_string)\n",
    "        \n",
    "        tour_data = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'venue': venues\n",
    "        })\n",
    "        \n",
    "        # Create links to setlists\n",
    "        tour_data['link'] = tour_data['date'].apply(\n",
    "            lambda x: f\"{base_url}setlists/{format_date_for_link(x)}a.asp\"\n",
    "        )\n",
    "        \n",
    "        # Apply special case links\n",
    "        tour_data['link'] = tour_data.apply(update_link, axis=1)\n",
    "        \n",
    "        # Filter out NODATA entries and get unique rows\n",
    "        tour_data = tour_data[tour_data['link'] != 'NODATA'].drop_duplicates()\n",
    "        \n",
    "        # Extract date components from link\n",
    "        tour_data['date_num'] = tour_data['link'].str.extract(r'(\\d+)')\n",
    "        tour_data['year'] = tour_data['date_num'].str[:4].astype(int)\n",
    "        tour_data['month'] = tour_data['date_num'].str[4:6].astype(int)\n",
    "        tour_data['day'] = tour_data['date_num'].str[6:8].astype(int)\n",
    "        \n",
    "        # Convert to proper date format\n",
    "        tour_data['date'] = pd.to_datetime(\n",
    "            tour_data.apply(\n",
    "                lambda x: f\"{x['month']:02d}/{x['day']:02d}/{x['year']:04d}\", \n",
    "                axis=1\n",
    "            ),\n",
    "            format=\"%m/%d/%Y\"\n",
    "        )\n",
    "        \n",
    "        # Process venue data\n",
    "        tour_data['venue_full'] = tour_data['venue']\n",
    "        # Extract venue name, city, state from venue string\n",
    "        venue_parts = tour_data['venue'].str.extract(r'(.*),\\s*(.*),\\s*(\\w{2}|[A-Z]{2})')\n",
    "        tour_data['venue_name'] = venue_parts[0]\n",
    "        tour_data['city'] = venue_parts[1]\n",
    "        tour_data['state'] = venue_parts[2]\n",
    "        \n",
    "        # Convert to uppercase for consistency\n",
    "        tour_data['city'] = tour_data['city'].str.upper()\n",
    "        tour_data['venue_name'] = tour_data['venue_name'].str.upper()\n",
    "        tour_data['venue_full'] = tour_data['venue_full'].str.upper()\n",
    "        \n",
    "        # Add year_index\n",
    "        tour_data = tour_data.reset_index(drop=True)\n",
    "        tour_data['year_index'] = tour_data.index + 1\n",
    "        \n",
    "        # Identify radio shows\n",
    "        tour_data['is_radio'] = tour_data['venue_full'].apply(is_radio_show)\n",
    "        \n",
    "        # Fix venue names and city names\n",
    "        tour_data['venue_name'] = tour_data.apply(correct_venue_name, axis=1)\n",
    "        tour_data['city'] = tour_data.apply(correct_city, axis=1)\n",
    "        \n",
    "        # Append to list\n",
    "        tour_df_list.append(tour_data)\n",
    "    \n",
    "    # Combine all years\n",
    "    combined_df = pd.concat(tour_df_list, ignore_index=True)\n",
    "    combined_df = combined_df.sort_values(['year', 'month', 'day']).reset_index(drop=True)\n",
    "    combined_df['show_index'] = combined_df.index + 1\n",
    "    \n",
    "    # Create Run Index + Show In Run Index (consecutive shows at same venue)\n",
    "    combined_df = combined_df.sort_values(['date', 'venue_name'])\n",
    "    combined_df['date_diff'] = combined_df['date'].diff().dt.days != 1\n",
    "    combined_df['venue_change'] = combined_df['venue_name'] != combined_df['venue_name'].shift()\n",
    "    combined_df['run_break'] = combined_df['date_diff'] | combined_df['venue_change']\n",
    "    combined_df['run_index'] = combined_df['run_break'].cumsum()\n",
    "    \n",
    "    # Calculate show_in_run (which show in a multi-night run)\n",
    "    run_groups = combined_df.groupby('run_index')\n",
    "    combined_df['min_show_index'] = run_groups['show_index'].transform('min')\n",
    "    combined_df['show_in_run'] = combined_df['show_index'] - combined_df['min_show_index'] + 1\n",
    "    \n",
    "    # Select and order final columns\n",
    "    result_df = combined_df[[\n",
    "        'link', 'date', 'date_num', 'year', 'month', 'day', \n",
    "        'state', 'city', 'venue_name', 'venue_full', \n",
    "        'run_index', 'show_index', 'show_in_run', 'year_index', \n",
    "        'venue', 'is_radio'\n",
    "    ]]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_set(row):\n",
    "    \"\"\"Identify which set a song belongs to\"\"\"\n",
    "    text = row[:3]\n",
    "    if text == \"??\":\n",
    "        return 'Details'\n",
    "    elif text == \"0: \":\n",
    "        return '0'\n",
    "    elif text == \"1: \":\n",
    "        return '1'\n",
    "    elif text == \"2: \":\n",
    "        return '2'\n",
    "    elif text == \"3: \":\n",
    "        return '3'\n",
    "    elif text == \"4: \":\n",
    "        return '4'\n",
    "    elif text == \"E: \":\n",
    "        return 'E'\n",
    "    elif text == \"E1:\":\n",
    "        return 'E'\n",
    "    elif text == \"E2:\":\n",
    "        return 'E'\n",
    "    elif text == \"E3:\":\n",
    "        return 'E'\n",
    "    elif re.match(r\"^\\d{2}/\", text):\n",
    "        return \"Details\"\n",
    "    elif row.startswith(\"*\"):\n",
    "        return \"Song_Notes\"\n",
    "    elif row.startswith(\"[\"):\n",
    "        return \"Show_Notes\"\n",
    "    else:\n",
    "        return \"Other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af007172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw(row):\n",
    "    \"\"\"Clean the raw text data\"\"\"\n",
    "    if row['set'] in ['0', '1', '2', '3', '4', 'E']:\n",
    "        return row['Raw'][3:]\n",
    "    elif row['set'] == 'Other':\n",
    "        return \"* \" + re.sub(r\".*\\*\", \"\", row['Raw'])\n",
    "    else:\n",
    "        return row['Raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song_name(name):\n",
    "    \"\"\"Standardize song names\"\"\"\n",
    "    if name in ['???', 'ARU/WSP JAM']:\n",
    "        return 'JAM'\n",
    "    elif name == 'THIS MUST BE THE PLACE (NA<EF>VE MELODY)':\n",
    "        return 'THIS MUST BE THE PLACE (NAIEVE MELODY)'\n",
    "    elif name == 'W<CR>M':\n",
    "        return 'WURM'\n",
    "    elif name in ['LAWYERS', 'GUNS', 'AND MONEY']:\n",
    "        return 'LAWYERS GUNS AND MONEY'\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_setlist(setlist_link):\n",
    "    \"\"\"Process a setlist from everydaycompanion.com and extract song information\"\"\"\n",
    "    setlist_link = setlist_link.lower()\n",
    "    \n",
    "    # Fetch and parse the HTML\n",
    "    response = requests.get(setlist_link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser', from_encoding='latin1')\n",
    "    \n",
    "    # Extract tables from the HTML\n",
    "    tables = soup.find_all('table')\n",
    "    if len(tables) < 6:\n",
    "        print(f\"Warning: Expected at least 6 tables, found {len(tables)} in {setlist_link}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract the setlist table (6th table)\n",
    "    setlist_table = tables[5]\n",
    "    rows = setlist_table.find_all('tr')\n",
    "    \n",
    "    # Extract text from each row\n",
    "    setlist_data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            setlist_data.append(cells[0].get_text(strip=True))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    setlist_raw = pd.DataFrame({'X1': setlist_data})\n",
    "    \n",
    "    # Clean and process the data\n",
    "    setlist_raw['X1'] = setlist_raw['X1'].str.replace('√Ø', 'i')\n",
    "    \n",
    "    # Determine the set for each row\n",
    "    setlist_raw['set'] = setlist_raw['X1'].apply(lambda x: determine_set(x))\n",
    "    setlist_raw = setlist_raw.rename(columns={\"X1\": \"Raw\"})\n",
    "    \n",
    "    # Clean the Raw column\n",
    "    setlist_raw['Raw'] = setlist_raw.apply(clean_raw, axis=1)\n",
    "    \n",
    "    # Create Songs DataFrame\n",
    "    songs = setlist_raw[~setlist_raw['set'].isin(['Details', 'Song_Notes', 'Show_Notes', 'Other'])].copy()\n",
    "    \n",
    "    # Process songs\n",
    "    song_rows = []\n",
    "    for _, row in songs.iterrows():\n",
    "        # Split by comma\n",
    "        for song_group in row['Raw'].split(','):\n",
    "            # Check for segues (\">\")\n",
    "            has_segue = \" > \" in song_group\n",
    "            into = 1 if has_segue else 0\n",
    "            \n",
    "            # Split by segue\n",
    "            for song in song_group.split(\" > \"):\n",
    "                song_name = song.strip().upper()\n",
    "                notes_id = song_name.count(\"*\")\n",
    "                song_notes_key = \"*\" * notes_id if notes_id > 0 else \"\"\n",
    "                \n",
    "                song_rows.append({\n",
    "                    'link': setlist_link,\n",
    "                    'set': row['set'],\n",
    "                    'song_name': re.sub(r\"\\*\", \"\", song_name),\n",
    "                    'into': into,\n",
    "                    'song_notes_key': song_notes_key,\n",
    "                    'notes_id': notes_id\n",
    "                })\n",
    "    \n",
    "    songs_df = pd.DataFrame(song_rows)\n",
    "    \n",
    "    # Clean song names   \n",
    "    songs_df['song_name'] = songs_df['song_name'].apply(clean_song_name)\n",
    "    \n",
    "    # Remove duplicates and add song_index\n",
    "    songs_df = songs_df.drop_duplicates()\n",
    "    songs_df['song_index'] = range(1, len(songs_df) + 1)\n",
    "    \n",
    "    # Extract raw notes\n",
    "    raw_notes = setlist_raw[setlist_raw['set'].isin([\"Song_Notes\", \"Show_Notes\", \"Other\"])].copy()\n",
    "    \n",
    "    # Process \"Other\" category notes\n",
    "    if \"Other\" in setlist_raw['set'].values:\n",
    "        other_rows = []\n",
    "        for _, row in raw_notes[raw_notes['set'] == 'Other'].iterrows():\n",
    "            for line in row['Raw'].split('\\r\\n'):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    if line.startswith(\"*\"):\n",
    "                        set_type = \"Song_Notes\"\n",
    "                    elif line.startswith(\"[\"):\n",
    "                        set_type = \"Show_Notes\"\n",
    "                    else:\n",
    "                        set_type = \"Other\"\n",
    "                    \n",
    "                    if set_type != \"Other\":\n",
    "                        other_rows.append({'Raw': line, 'set': set_type})\n",
    "        \n",
    "        other_df = pd.DataFrame(other_rows)\n",
    "        raw_notes = pd.concat([raw_notes[raw_notes['set'] != 'Other'], other_df], ignore_index=True)\n",
    "    \n",
    "    # Create Show Notes DataFrame\n",
    "    if \"Show_Notes\" in raw_notes['set'].values:\n",
    "        show_notes = raw_notes[raw_notes['set'] == \"Show_Notes\"]['Raw'].tolist()\n",
    "        show_notes_df = pd.DataFrame({\n",
    "            'link': [setlist_link],\n",
    "            'show_notes': [\" \".join(show_notes)]\n",
    "        })\n",
    "    else:\n",
    "        show_notes_df = pd.DataFrame({\n",
    "            'link': [setlist_link],\n",
    "            'show_notes': [\"\"]\n",
    "        })\n",
    "    \n",
    "    # Create Notes DataFrame\n",
    "    if \"Song_Notes\" in raw_notes['set'].values:\n",
    "        notes_str = \" \".join(raw_notes[raw_notes['set'] == 'Song_Notes']['Raw'].tolist())\n",
    "        \n",
    "        # Split notes by asterisk pattern\n",
    "        notes_split = re.split(r'(?<=[A-Za-z])\\*', notes_str)\n",
    "        \n",
    "        for i in range(len(notes_split)):\n",
    "            if i > 0:\n",
    "                notes_split[i] = \"*\" + notes_split[i]\n",
    "        \n",
    "        notes_rows = []\n",
    "        for note in notes_split:\n",
    "            if note.strip():\n",
    "                parts = note.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    notes_rows.append({\n",
    "                        'link': setlist_link,\n",
    "                        'song_notes_key': parts[0].strip(),\n",
    "                        'song_note_detail': parts[1].strip().upper()\n",
    "                    })\n",
    "                else:\n",
    "                    notes_rows.append({\n",
    "                        'link': setlist_link,\n",
    "                        'song_notes_key': parts[0].strip(),\n",
    "                        'song_note_detail': \"\"\n",
    "                    })\n",
    "        \n",
    "        notes_df = pd.DataFrame(notes_rows)\n",
    "    else:\n",
    "        notes_df = pd.DataFrame({\n",
    "            'link': [setlist_link],\n",
    "            'song_notes_key': [None],\n",
    "            'song_note_detail': [\"\"]\n",
    "        })\n",
    "    \n",
    "    # Join Notes To Songs\n",
    "    songs_df = songs_df.merge(show_notes_df, on='link', how='left')\n",
    "    songs_df = songs_df.merge(notes_df, on=['link', 'song_notes_key'], how='left')\n",
    "    \n",
    "    # Select final columns\n",
    "    songs_df = songs_df.drop(columns=['song_notes_key', 'notes_id'])\n",
    "    \n",
    "    print(f\"Now Loading {setlist_link}\")\n",
    "    \n",
    "    return songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_soundcheck(notes):\n",
    "    \"\"\"Check if a show is a soundcheck\"\"\"\n",
    "    if pd.isna(notes):\n",
    "        return 0\n",
    "    return 1 if '[Soundcheck; ' in notes else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_opening_act(notes):\n",
    "    \"\"\"Check if Widespread Panic was an opening act\"\"\"\n",
    "    if pd.isna(notes):\n",
    "        return 0\n",
    "    return 1 if 'opened for' in str(notes).lower() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(start=1986, end=2025, max_workers=4):\n",
    "    \"\"\"\n",
    "    Load all Widespread Panic show information from EveryDayCompanion\n",
    "    \n",
    "    Args:\n",
    "        start (int): Start year\n",
    "        end (int): End year\n",
    "        max_workers (int): Maximum number of parallel workers for fetching setlists\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (songs DataFrame, historical shows DataFrame, future shows DataFrame)\n",
    "    \"\"\"\n",
    "    # Load Dim Stage 1\n",
    "    print(f\"Loading tour data from {start} to {end}...\")\n",
    "    tour_data = process_dim(st_yr=start, end_yr=end)\n",
    "    \n",
    "    # Split Historical and Future\n",
    "    today = datetime.now().date()\n",
    "    show_dim = tour_data[(tour_data['date'].dt.date < today) & \n",
    "                         (tour_data['year'] >= start) & \n",
    "                         (tour_data['year'] <= end)]\n",
    "    fut_dim = tour_data[tour_data['date'].dt.date >= today]\n",
    "    \n",
    "    # Peek\n",
    "    print(f\"{len(show_dim)} Historical & {len(fut_dim)} Future Shows And EDC Links Loaded - Now Loading Setlists\")\n",
    "    print(show_dim.sort_values('show_index', ascending=False).head())\n",
    "    \n",
    "    # Load Setlists\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    songs_list = []\n",
    "    links = show_dim['link'].tolist()\n",
    "    \n",
    "    # Process setlists in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_link = {executor.submit(process_setlist, link): link for link in links}\n",
    "        \n",
    "        # Process as they complete\n",
    "        for future in tqdm(as_completed(future_to_link), total=len(links), desc=\"Processing setlists\"):\n",
    "            link = future_to_link[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if not result.empty:\n",
    "                    songs_list.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"{link} generated an exception: {exc}\")\n",
    "    \n",
    "    # Combine all song dataframes\n",
    "    if songs_list:\n",
    "        songs = pd.concat(songs_list, ignore_index=True)\n",
    "    else:\n",
    "        songs = pd.DataFrame()\n",
    "    \n",
    "    # Process Songs\n",
    "    if not songs.empty:\n",
    "        songs['song_note_detail'] = songs['song_note_detail'].apply(\n",
    "            lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "        )\n",
    "        songs['show_notes'] = songs['show_notes'].apply(\n",
    "            lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "        )\n",
    "        \n",
    "        # Convert set to numeric\n",
    "        songs['set_num'] = songs['set'].apply(lambda x: \"99\" if x == 'E' else x)\n",
    "        songs['set'] = pd.to_numeric(songs['set_num'])\n",
    "        \n",
    "        # Handle special set numbering\n",
    "        songs_grouped = songs.groupby('link')\n",
    "        \n",
    "        # Calculate min and max set for each show\n",
    "        min_max_sets = songs_grouped.agg({'set': ['min', 'max']})\n",
    "        min_max_sets.columns = ['min_set', 'max_set']\n",
    "        min_max_sets = min_max_sets.reset_index()\n",
    "        \n",
    "        # Merge back to songs\n",
    "        songs = songs.merge(min_max_sets, on='link')\n",
    "        \n",
    "        # Adjust set numbers\n",
    "        songs['set'] = songs.apply(\n",
    "            lambda row: 1 if (row['set'] == 0 and row['min_set'] == 0 and \n",
    "                              row['max_set'] in [99, 0]) else row['set'], axis=1\n",
    "        )\n",
    "        \n",
    "        # Select final columns\n",
    "        songs = songs[['link', 'set', 'song_index', 'song_name', 'into', 'song_note_detail', 'show_notes']]\n",
    "        \n",
    "        # Create dim_songs\n",
    "        dim_songs = songs.groupby(['link', 'show_notes']).agg({'song_index': 'max'}).reset_index()\n",
    "        dim_songs = dim_songs.rename(columns={'song_index': 'n_songs'})\n",
    "        \n",
    "        # Remove show_notes from songs\n",
    "        songs = songs.drop(columns=['show_notes'])\n",
    "    else:\n",
    "        dim_songs = pd.DataFrame(columns=['link', 'show_notes', 'n_songs'])\n",
    "    \n",
    "    # Process Show Information\n",
    "    # Slim Future\n",
    "    slim_fut = fut_dim.copy()\n",
    "    slim_fut['is_soundcheck'] = 0\n",
    "    slim_fut['is_opening_act'] = 0\n",
    "    slim_fut['show_notes'] = \"\"\n",
    "    slim_fut['n_songs'] = 0\n",
    "    slim_fut['weekday'] = slim_fut['date'].dt.day_name()\n",
    "    slim_fut['is_fut'] = 1\n",
    "    slim_fut = slim_fut.sort_values('date')\n",
    "    \n",
    "    # Process historical shows\n",
    "    dim = show_dim.merge(dim_songs, on='link', how='left')\n",
    "    \n",
    "    # Identify radio shows, soundchecks, and opening acts\n",
    "    dim['is_radio'] = dim['venue_full'].apply(is_radio_show)\n",
    "    dim['is_soundcheck'] = dim['show_notes'].apply(is_soundcheck)\n",
    "    dim['is_opening_act'] = dim['show_notes'].apply(is_opening_act)\n",
    "    dim['weekday'] = dim['date'].dt.day_name()\n",
    "    dim['is_fut'] = 0\n",
    "    \n",
    "    # Fill NaN values\n",
    "    dim['n_songs'] = dim['n_songs'].fillna(0)\n",
    "    dim['show_notes'] = dim['show_notes'].fillna(\"\")\n",
    "    \n",
    "    # Sort and combine with future shows\n",
    "    dim = dim.sort_values('show_index')\n",
    "    combined_dim = pd.concat([dim, slim_fut], ignore_index=True)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    \n",
    "    # Print summary\n",
    "    print(f'Successfully Loaded {len(combined_dim[combined_dim[\"is_fut\"] == 0][\"link\"].unique())} '\n",
    "          f'Widespread Panic Shows ({len(songs)} Total Songs) in {elapsed_time:.2f} Minutes '\n",
    "          f'From {combined_dim[\"year\"].min()} to {combined_dim[\"year\"].max()}')\n",
    "    \n",
    "    # Return as tuple\n",
    "    return (\n",
    "        songs,\n",
    "        combined_dim[combined_dim['is_fut'] == 0],\n",
    "        combined_dim[combined_dim['is_fut'] == 1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_all_data(max_workers=4):\n",
    "    \"\"\"Update the most recent Widespread Panic shows data\"\"\"\n",
    "    # Check paths\n",
    "    song_path = './Data/WSP_Song_FactTable_1986_to_2024.pkl'\n",
    "    dim_hist_path = './Data/WSP_Dim_Show_Historical_1986_to_2024.pkl'\n",
    "    dim_fut_path = './Data/WSP_Dim_Show_Future_2024_to_2024.pkl'\n",
    "    \n",
    "    # Check if data files exist\n",
    "    if not all(os.path.exists(path) for path in [song_path, dim_hist_path, dim_fut_path]):\n",
    "        print(\"Data files not found. Please run load_all_data first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load previous data\n",
    "    with open(dim_hist_path, 'rb') as f:\n",
    "        prev_dim_hist = pickle.load(f)\n",
    "    \n",
    "    with open(dim_fut_path, 'rb') as f:\n",
    "        prev_dim_fut = pickle.load(f)\n",
    "    \n",
    "    # Set Up Dim For Update\n",
    "    last_show = prev_dim_hist['date'].max()\n",
    "    tour_data = process_dim(st_yr=1986, end_yr=2025)\n",
    "    \n",
    "    today = datetime.now().date()\n",
    "    update_dim = tour_data[(tour_data['date'].dt.date < today) & (tour_data['date'] > last_show)]\n",
    "    fut_dim = tour_data[tour_data['date'].dt.date >= today]\n",
    "    \n",
    "    if len(update_dim) == 0:\n",
    "        print(\"All Historical Shows Up to Date\")\n",
    "        with open(song_path, 'rb') as f:\n",
    "            songs = pickle.load(f)\n",
    "        with open(dim_hist_path, 'rb') as f:\n",
    "            dim_hist = pickle.load(f)\n",
    "        return (songs, dim_hist, fut_dim)\n",
    "    else:\n",
    "        # Load previous songs\n",
    "        with open(song_path, 'rb') as f:\n",
    "            prev_song = pickle.load(f)\n",
    "        \n",
    "        # Find links to load\n",
    "        prev_links = set(prev_song['link'].unique())\n",
    "        load_links = [link for link in update_dim[update_dim['date'] > last_show]['link'].unique() \n",
    "                     if link not in prev_links]\n",
    "        \n",
    "        # Process setlists in parallel\n",
    "        update_songs_list = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_link = {executor.submit(process_setlist, link): link for link in load_links}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_link), total=len(load_links), desc=\"Processing new setlists\"):\n",
    "                link = future_to_link[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if not result.empty:\n",
    "                        update_songs_list.append(result)\n",
    "                except Exception as exc:\n",
    "                    print(f\"{link} generated an exception: {exc}\")\n",
    "        \n",
    "        # Combine all new song dataframes\n",
    "        if update_songs_list:\n",
    "            update_songs = pd.concat(update_songs_list, ignore_index=True)\n",
    "        else:\n",
    "            update_songs = pd.DataFrame()\n",
    "        \n",
    "        # Process new songs and merge with existing data\n",
    "        if not update_songs.empty:\n",
    "            # Process the new songs (same as in load_all_data)\n",
    "            update_songs['song_note_detail'] = update_songs['song_note_detail'].apply(\n",
    "                lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "            )\n",
    "            update_songs['show_notes'] = update_songs['show_notes'].apply(\n",
    "                lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "            )\n",
    "            \n",
    "            # Convert set to numeric\n",
    "            update_songs['set_num'] = update_songs['set'].apply(lambda x: \"99\" if x == 'E' else x)\n",
    "            update_songs['set'] = pd.to_numeric(update_songs['set_num'])\n",
    "            \n",
    "            # Handle special set numbering\n",
    "            songs_grouped = update_songs.groupby('link')\n",
    "            min_max_sets = songs_grouped.agg({'set': ['min', 'max']})\n",
    "            min_max_sets.columns = ['min_set', 'max_set']\n",
    "            min_max_sets = min_max_sets.reset_index()\n",
    "            update_songs = update_songs.merge(min_max_sets, on='link')\n",
    "            \n",
    "            # Adjust set numbers\n",
    "            update_songs['set'] = update_songs.apply(\n",
    "                lambda row: 1 if (row['set'] == 0 and row['min_set'] == 0 and \n",
    "                                 row['max_set'] in [99, 0]) else row['set'], axis=1\n",
    "            )\n",
    "            \n",
    "            # Select final columns and combine with previous songs\n",
    "            update_songs = update_songs[['link', 'set', 'song_index', 'song_name', 'into', 'song_note_detail', 'show_notes']]\n",
    "            dim_songs = update_songs.groupby(['link', 'show_notes']).agg({'song_index': 'max'}).reset_index()\n",
    "            dim_songs = dim_songs.rename(columns={'song_index': 'n_songs'})\n",
    "            \n",
    "            all_songs = pd.concat([prev_song, update_songs.drop(columns=['show_notes'])], ignore_index=True)\n",
    "        else:\n",
    "            dim_songs = pd.DataFrame(columns=['link', 'show_notes', 'n_songs'])\n",
    "            all_songs = prev_song\n",
    "        \n",
    "        # Process Show Information\n",
    "        # Prepare future shows\n",
    "        slim_fut = fut_dim.copy()\n",
    "        slim_fut['is_soundcheck'] = 0\n",
    "        slim_fut['is_opening_act'] = 0\n",
    "        slim_fut['show_notes'] = \"\"\n",
    "        slim_fut['n_songs'] = 0\n",
    "        slim_fut['weekday'] = slim_fut['date'].dt.day_name()\n",
    "        slim_fut['is_fut'] = 1\n",
    "        \n",
    "        # Process new historical shows\n",
    "        new_dim = update_dim.merge(dim_songs, on='link', how='left')\n",
    "        new_dim['is_radio'] = new_dim['venue_full'].apply(is_radio_show)\n",
    "        new_dim['is_soundcheck'] = new_dim['show_notes'].apply(is_soundcheck)\n",
    "        new_dim['is_opening_act'] = new_dim['show_notes'].apply(is_opening_act)\n",
    "        new_dim['weekday'] = new_dim['date'].dt.day_name()\n",
    "        new_dim['is_fut'] = 0\n",
    "        new_dim['n_songs'] = new_dim['n_songs'].fillna(0)\n",
    "        new_dim['show_notes'] = new_dim['show_notes'].fillna(\"\")\n",
    "        \n",
    "        # Combine with previous historical data\n",
    "        historical_dim = pd.concat([prev_dim_hist, new_dim], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3ef72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_radio_show(venue_full):\n",
    "    if pd.isna(venue_full):\n",
    "        return 0\n",
    "    if re.search(r'\\b\\d+\\.\\d+FM\\b', venue_full):\n",
    "        return 1\n",
    "    if re.search(r'\\b\\d+\\.\\d\\b', venue_full):\n",
    "        return 1\n",
    "    if any(term in venue_full for term in [\n",
    "        'NBC STUDIOS', 'ED SULLIVAN THEATER', \n",
    "        'STUDIO 6B, ROCKAFELLER CENTER', 'CNN STUDIOS',\n",
    "        ' STUDIO', ' RECORD'\n",
    "    ]):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c710331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(start=1986, end=2025, max_workers=4):\n",
    "    \"\"\"\n",
    "    Load all Widespread Panic show information from EveryDayCompanion\n",
    "    \n",
    "    Args:\n",
    "        start (int): Start year\n",
    "        end (int): End year\n",
    "        max_workers (int): Maximum number of parallel workers for fetching setlists\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (songs DataFrame, historical shows DataFrame, future shows DataFrame)\n",
    "    \"\"\"\n",
    "    # Load Dim Stage 1\n",
    "    print(f\"Loading tour data from {start} to {end}...\")\n",
    "    tour_data = process_dim(st_yr=start, end_yr=end)\n",
    "    \n",
    "    # Split Historical and Future\n",
    "    today = datetime.datetime.now().date()\n",
    "    show_dim = tour_data[(tour_data['date'].dt.date < today) & \n",
    "                         (tour_data['year'] >= start) & \n",
    "                         (tour_data['year'] <= end)]\n",
    "    fut_dim = tour_data[tour_data['date'].dt.date >= today]\n",
    "    \n",
    "    # Peek\n",
    "    print(f\"{len(show_dim)} Historical & {len(fut_dim)} Future Shows And EDC Links Loaded - Now Loading Setlists\")\n",
    "    print(show_dim.sort_values('show_index', ascending=False).head())\n",
    "    \n",
    "    # Load Setlists\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    songs_list = []\n",
    "    links = show_dim['link'].tolist()\n",
    "    \n",
    "    # Process setlists in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create a dictionary of futures to links\n",
    "        future_to_link = {executor.submit(process_setlist, link): link for link in links}\n",
    "        \n",
    "        # Process as they complete\n",
    "        for future in tqdm(as_completed(future_to_link), total=len(links), desc=\"Processing setlists\"):\n",
    "            link = future_to_link[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if not result.empty:\n",
    "                    songs_list.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"{link} generated an exception: {exc}\")\n",
    "    \n",
    "    # Combine all song dataframes\n",
    "    if songs_list:\n",
    "        songs = pd.concat(songs_list, ignore_index=True)\n",
    "    else:\n",
    "        songs = pd.DataFrame()\n",
    "    \n",
    "    # Process Songs\n",
    "    if not songs.empty:\n",
    "        songs['song_note_detail'] = songs['song_note_detail'].apply(\n",
    "            lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "        )\n",
    "        songs['show_notes'] = songs['show_notes'].apply(\n",
    "            lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "        )\n",
    "        \n",
    "        # Convert set to numeric\n",
    "        songs['set_num'] = songs['set'].apply(lambda x: \"99\" if x == 'E' else x)\n",
    "        songs['set'] = pd.to_numeric(songs['set_num'])\n",
    "        \n",
    "        # Handle special set numbering\n",
    "        songs_grouped = songs.groupby('link')\n",
    "        \n",
    "        # Calculate min and max set for each show\n",
    "        min_max_sets = songs_grouped.agg({'set': ['min', 'max']})\n",
    "        min_max_sets.columns = ['min_set', 'max_set']\n",
    "        min_max_sets = min_max_sets.reset_index()\n",
    "        \n",
    "        # Merge back to songs\n",
    "        songs = songs.merge(min_max_sets, on='link')\n",
    "        \n",
    "        # Adjust set numbers\n",
    "        songs['set'] = songs.apply(\n",
    "            lambda row: 1 if (row['set'] == 0 and row['min_set'] == 0 and \n",
    "                              row['max_set'] in [99, 0]) else row['set'], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Select final columns\n",
    "        songs = songs[['link', 'set', 'song_index', 'song_name', 'into', 'song_note_detail', 'show_notes']]\n",
    "        \n",
    "        # Create dim_songs\n",
    "        dim_songs = songs.groupby(['link', 'show_notes']).agg({'song_index': 'max'}).reset_index()\n",
    "        dim_songs = dim_songs.rename(columns={'song_index': 'n_songs'})\n",
    "        \n",
    "        # Remove show_notes from songs\n",
    "        songs = songs.drop(columns=['show_notes'])\n",
    "    else:\n",
    "        dim_songs = pd.DataFrame(columns=['link', 'show_notes', 'n_songs'])\n",
    "    \n",
    "    # Process Show Information\n",
    "    # Slim Future\n",
    "    slim_fut = fut_dim.copy()\n",
    "    slim_fut['is_soundcheck'] = 0\n",
    "    slim_fut['is_opening_act'] = 0\n",
    "    slim_fut['show_notes'] = \"\"\n",
    "    slim_fut['n_songs'] = 0\n",
    "    slim_fut['weekday'] = slim_fut['date'].dt.day_name()\n",
    "    slim_fut['is_fut'] = 1\n",
    "    slim_fut = slim_fut.sort_values('date')\n",
    "    \n",
    "    # Process historical shows\n",
    "    dim = show_dim.merge(dim_songs, on='link', how='left')\n",
    "    \n",
    "    # Identify radio shows, soundchecks, and opening acts\n",
    "    dim['is_radio'] = dim['venue_full'].apply(is_radio_show)\n",
    "    dim['is_soundcheck'] = dim['show_notes'].apply(is_soundcheck)\n",
    "    dim['is_opening_act'] = dim['show_notes'].apply(is_opening_act)\n",
    "    dim['weekday'] = dim['date'].dt.day_name()\n",
    "    dim['is_fut'] = 0\n",
    "    \n",
    "    # Fill NaN values\n",
    "    dim['n_songs'] = dim['n_songs'].fillna(0)\n",
    "    dim['show_notes'] = dim['show_notes'].fillna(\"\")\n",
    "    \n",
    "    # Sort and combine with future shows\n",
    "    dim = dim.sort_values('show_index')\n",
    "    combined_dim = pd.concat([dim, slim_fut], ignore_index=True)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    \n",
    "    # Print summary\n",
    "    print(f'Successfully Loaded {len(combined_dim[combined_dim[\"is_fut\"] == 0][\"link\"].unique())} '\n",
    "          f'Widespread Panic Shows ({len(songs)} Total Songs) in {elapsed_time:.2f} Minutes '\n",
    "          f'From {combined_dim[\"year\"].min()} to {combined_dim[\"year\"].max()}')\n",
    "    \n",
    "    # Return as tuple\n",
    "    return (\n",
    "        songs,\n",
    "        combined_dim[combined_dim['is_fut'] == 0],\n",
    "        combined_dim[combined_dim['is_fut'] == 1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_all_data(max_workers=4):\n",
    "    \"\"\"\n",
    "    Update the most recent Widespread Panic shows data\n",
    "    \n",
    "    Args:\n",
    "        max_workers (int): Maximum number of parallel workers for fetching setlists\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (songs DataFrame, historical shows DataFrame, future shows DataFrame)\n",
    "    \"\"\"\n",
    "    # Load Previous Data\n",
    "    print(\"fix paths first\")\n",
    "    song_path = './Data/WSP_Song_FactTable_1986_to_2024.pkl'\n",
    "    dim_hist_path = './Data/WSP_Dim_Show_Historical_1986_to_2024.pkl'\n",
    "    dim_fut_path = './Data/WSP_Dim_Show_Future_2024_to_2024.pkl'\n",
    "    \n",
    "    # Check if data files exist\n",
    "    if not all(os.path.exists(path) for path in [song_path, dim_hist_path, dim_fut_path]):\n",
    "        print(\"Data files not found. Please run load_all_data first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load previous data\n",
    "    with open(dim_hist_path, 'rb') as f:\n",
    "        prev_dim_hist = pickle.load(f)\n",
    "    \n",
    "    with open(dim_fut_path, 'rb') as f:\n",
    "        prev_dim_fut = pickle.load(f)\n",
    "    \n",
    "    # Set Up Dim For Update\n",
    "    last_show = prev_dim_hist['date'].max()\n",
    "    tour_data = process_dim(st_yr=1986, end_yr=2025)\n",
    "    \n",
    "    today = datetime.datetime.now().date()\n",
    "    update_dim = tour_data[(tour_data['date'].dt.date < today) & (tour_data['date'] > last_show)]\n",
    "    fut_dim = tour_data[tour_data['date'].dt.date >= today]\n",
    "    \n",
    "    if len(update_dim) == 0:\n",
    "        print(\"All Historical Shows Up to Date\")\n",
    "        with open(song_path, 'rb') as f:\n",
    "            songs = pickle.load(f)\n",
    "        with open(dim_hist_path, 'rb') as f:\n",
    "            dim_hist = pickle.load(f)\n",
    "        return (songs, dim_hist, fut_dim)\n",
    "    else:\n",
    "        # Peek\n",
    "        print(f\"Now Updating {len(update_dim)} Shows | {len(fut_dim)} Future Shows And EDC Links Loaded - Now Loading Setlists\")\n",
    "        print(update_dim.sort_values('show_index', ascending=False).head())\n",
    "        \n",
    "        # Load Setlists\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with open(song_path, 'rb') as f:\n",
    "            prev_song = pickle.load(f)\n",
    "        \n",
    "        # Find links to load\n",
    "        prev_links = set(prev_song['link'].unique())\n",
    "        load_links = [link for link in update_dim[update_dim['date'] > last_show]['link'].unique() \n",
    "                     if link not in prev_links]\n",
    "        \n",
    "        # Process setlists in parallel\n",
    "        update_songs_list = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_link = {executor.submit(process_setlist, link): link for link in load_links}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_link), total=len(load_links), desc=\"Processing new setlists\"):\n",
    "                link = future_to_link[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if not result.empty:\n",
    "                        update_songs_list.append(result)\n",
    "                except Exception as exc:\n",
    "                    print(f\"{link} generated an exception: {exc}\")\n",
    "        \n",
    "        # Combine all new song dataframes\n",
    "        if update_songs_list:\n",
    "            update_songs = pd.concat(update_songs_list, ignore_index=True)\n",
    "        else:\n",
    "            update_songs = pd.DataFrame()\n",
    "        \n",
    "        # Process Songs\n",
    "        if not update_songs.empty:\n",
    "            # Clean up song data\n",
    "            update_songs['song_note_detail'] = update_songs['song_note_detail'].apply(\n",
    "                lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "            )\n",
    "            update_songs['show_notes'] = update_songs['show_notes'].apply(\n",
    "                lambda x: np.nan if pd.isna(x) or x == \"\" else x\n",
    "            )\n",
    "            \n",
    "            # Convert set to numeric\n",
    "            update_songs['set_num'] = update_songs['set'].apply(lambda x: \"99\" if x == 'E' else x)\n",
    "            update_songs['set'] = pd.to_numeric(update_songs['set_num'])\n",
    "            \n",
    "            # Handle special set numbering\n",
    "            songs_grouped = update_songs.groupby('link')\n",
    "            \n",
    "            # Calculate min and max set for each show\n",
    "            min_max_sets = songs_grouped.agg({'set': ['min', 'max']})\n",
    "            min_max_sets.columns = ['min_set', 'max_set']\n",
    "            min_max_sets = min_max_sets.reset_index()\n",
    "            \n",
    "            # Merge back to songs\n",
    "            update_songs = update_songs.merge(min_max_sets, on='link')\n",
    "            \n",
    "            # Adjust set numbers\n",
    "            update_songs['set'] = update_songs.apply(\n",
    "                lambda row: 1 if (row['set'] == 0 and row['min_set'] == 0 and \n",
    "                                  row['max_set'] in [99, 0]) else row['set'], \n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Select final columns\n",
    "            update_songs = update_songs[['link', 'set', 'song_index', 'song_name', 'into', 'song_note_detail', 'show_notes']]\n",
    "            \n",
    "            # Create dim_songs\n",
    "            dim_songs = update_songs.groupby(['link', 'show_notes']).agg({'song_index': 'max'}).reset_index()\n",
    "            dim_songs = dim_songs.rename(columns={'song_index': 'n_songs'})\n",
    "            \n",
    "            # Combine with previous songs\n",
    "            all_songs = pd.concat([prev_song, update_songs.drop(columns=['show_notes'])], ignore_index=True)\n",
    "        else:\n",
    "            dim_songs = pd.DataFrame(columns=['link', 'show_notes', 'n_songs'])\n",
    "            all_songs = prev_song\n",
    "        \n",
    "        # Process Show Information\n",
    "        # Slim Future\n",
    "        slim_fut = fut_dim.copy()\n",
    "        slim_fut['is_soundcheck'] = 0\n",
    "        slim_fut['is_opening_act'] = 0\n",
    "        slim_fut['show_notes'] = \"\"\n",
    "        slim_fut['n_songs'] = 0\n",
    "        slim_fut['weekday'] = slim_fut['date'].dt.day_name()\n",
    "        slim_fut['is_fut'] = 1\n",
    "        slim_fut = slim_fut.sort_values('date')\n",
    "        \n",
    "        # Process new shows\n",
    "        new_dim = update_dim.merge(dim_songs, on='link', how='left')\n",
    "        \n",
    "        # Identify radio shows, soundchecks, and opening acts\n",
    "        def is_radio_show(venue_full):\n",
    "            if pd.isna(venue_full):\n",
    "                return 0\n",
    "            if re.search(r'\\b\\d+\\.\\d+FM\\b', venue_full):\n",
    "                return 1\n",
    "            if re.search(r'\\b\\d+\\.\\d\\b', venue_full):\n",
    "                return 1\n",
    "            if any(term in venue_full for term in [\n",
    "                'NBC STUDIOS', 'ED SULLIVAN THEATER', \n",
    "                'STUDIO 6B, ROCKAFELLER CENTER', 'CNN STUDIOS',\n",
    "                ' STUDIO', ' RECORD'\n",
    "            ]):\n",
    "                return 1\n",
    "            return 0\n",
    "        \n",
    "        def is_soundcheck(notes):\n",
    "            if pd.isna(notes):\n",
    "                return 0\n",
    "            return 1 if '[Soundcheck; ' in notes else 0\n",
    "        \n",
    "        def is_opening_act(notes):\n",
    "            if pd.isna(notes):\n",
    "                return 0\n",
    "            return 1 if 'opened for' in str(notes).lower() else 0\n",
    "        \n",
    "        new_dim['is_radio'] = new_dim['venue_full'].apply(is_radio_show)\n",
    "        new_dim['is_soundcheck'] = new_dim['show_notes'].apply(is_soundcheck)\n",
    "        new_dim['is_opening_act'] = new_dim['show_notes'].apply(is_opening_act)\n",
    "        new_dim['weekday'] = new_dim['date'].dt.day_name()\n",
    "        new_dim['is_fut'] = 0\n",
    "        \n",
    "        # Fill NaN values\n",
    "        new_dim['n_songs'] = new_dim['n_songs'].fillna(0)\n",
    "        new_dim['show_notes'] = new_dim['show_notes'].fillna(\"\")\n",
    "        \n",
    "        # Sort and combine with previous historical and future shows\n",
    "        new_dim = new_dim.sort_values('show_index')\n",
    "        dim = pd.concat([prev_dim_hist, new_dim], ignore_index=True)\n",
    "        \n",
    "        # Split into historical and future\n",
    "        historical_dim = dim[dim['is_fut'] == 0]\n",
    "        future_dim = slim_fut\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time) / 60\n",
    "        \n",
    "        # Print summary\n",
    "        print(f'Successfully Updated {len(new_dim[\"link\"].unique())} '\n",
    "              f'Widespread Panic Shows ({len(update_songs) if not update_songs.empty else 0} Total Songs) in {elapsed_time:.2f} Minutes '\n",
    "              f'From {new_dim[\"date\"].min()} to {new_dim[\"date\"].max()}')\n",
    "        \n",
    "        # Return as tuple\n",
    "        return (all_songs, historical_dim, future_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_setlists(song_df, all_df):\n",
    "    \"\"\"\n",
    "    Save the processed data to pickle files\n",
    "    \n",
    "    Args:\n",
    "        song_df (DataFrame): Song fact table\n",
    "        all_df (DataFrame): Combined show dimension table\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    Path(\"./Data\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract year range from links\n",
    "    start_year = min(int(link[38:42]) for link in all_df['link'] if isinstance(link, str))\n",
    "    end_year = max(int(link[38:42]) for link in all_df['link'] if isinstance(link, str))\n",
    "    \n",
    "    # Create paths\n",
    "    fact_path = f'./Data/WSP_Song_FactTable_{start_year}_to_{end_year}.pkl'\n",
    "    dim_hist_path = f'./Data/WSP_Dim_Show_Historical_{start_year}_to_{end_year}.pkl'\n",
    "    dim_fut_path = f'./Data/WSP_Dim_Show_Future_{end_year}_to_{end_year}.pkl'\n",
    "    dim_path = f'./Data/WSP_Show_Dim_Table_{start_year}_to_{end_year}.pkl'\n",
    "    \n",
    "    # Save data\n",
    "    with open(fact_path, 'wb') as f:\n",
    "        pickle.dump(song_df, f)\n",
    "    \n",
    "    with open(dim_path, 'wb') as f:\n",
    "        pickle.dump(all_df, f)\n",
    "    \n",
    "    # Also save historical and future separately for update function\n",
    "    historical_df = all_df[all_df['is_fut'] == 0]\n",
    "    future_df = all_df[all_df['is_fut'] == 1]\n",
    "    \n",
    "    with open(dim_hist_path, 'wb') as f:\n",
    "        pickle.dump(historical_df, f)\n",
    "    \n",
    "    with open(dim_fut_path, 'wb') as f:\n",
    "        pickle.dump(future_df, f)\n",
    "    \n",
    "    # Print confirmation\n",
    "    print(f\"Setlist Data Saved To {fact_path}\")\n",
    "    print(f\"Show Data Saved To {dim_path}\")\n",
    "    print(f\"Historical Show Data Saved To {dim_hist_path}\")\n",
    "    print(f\"Future Show Data Saved To {dim_fut_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aeca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Choose whether to load all data or just update\n",
    "    load_all = True  # Set to False to update instead\n",
    "    \n",
    "    if load_all:\n",
    "        # Load All From Scratch\n",
    "        print(\"Loading all data from scratch...\")\n",
    "        data_list = load_all_data(start=1986, end=2024)\n",
    "    else:\n",
    "        # Update Setlist Data\n",
    "        print(\"Updating with recent shows...\")\n",
    "        data_list = update_all_data()\n",
    "    \n",
    "    if data_list is None:\n",
    "        print(\"Error: Failed to load or update data\")\n",
    "        return\n",
    "    \n",
    "    # Create Tables\n",
    "    # Song Fact Table\n",
    "    fact_song = data_list[0]\n",
    "    \n",
    "    # Dim Historical Show (All Show Data Related To Historical Concerts)\n",
    "    dim_historical = data_list[1]\n",
    "    \n",
    "    # Dim Future Show (All Show Data Related To Future Concerts)\n",
    "    if len(data_list[2]) > 0:\n",
    "        dim_future = data_list[2]\n",
    "    else:\n",
    "        # Load manual future shows if everydaycompanion not updated\n",
    "        future_csv_path = \"./Data/20250209_PanicFutureDim - FutureDim.csv\"\n",
    "        if os.path.exists(future_csv_path):\n",
    "            dim_future = pd.read_csv(future_csv_path)\n",
    "            dim_future['date_num'] = dim_future['date_num'].astype(str)\n",
    "            dim_future['show_notes'] = dim_future['show_notes'].fillna(\"\")\n",
    "            \n",
    "            # Filter to only future shows\n",
    "            today = datetime.datetime.now().date()\n",
    "            dim_future = dim_future[pd.to_datetime(dim_future['date']).dt.date >= today]\n",
    "        else:\n",
    "            print(f\"Warning: {future_csv_path} not found. Using empty future shows DataFrame.\")\n",
    "            dim_future = pd.DataFrame()\n",
    "    \n",
    "    # Combine All\n",
    "    dim_all = pd.concat([dim_historical, dim_future], ignore_index=True)\n",
    "    \n",
    "    # Save Tables\n",
    "    save_setlists(fact_song, dim_all)\n",
    "    \n",
    "    print(f\"Processed {len(fact_song)} songs across {len(dim_historical)} historical shows\")\n",
    "    print(f\"Future shows: {len(dim_future)}\")\n",
    "    \n",
    "    return fact_song, dim_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130bb3a",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fact_song, dim_all = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
